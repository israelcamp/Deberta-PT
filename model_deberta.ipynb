{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaV2Tokenizer, AutoTokenizer, DebertaV2ForMaskedLM\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"c4ai-wik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deberta_pt_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForMaskedLM: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized because the shapes did not match:\n",
      "- deberta.embeddings.word_embeddings.weight: found shape torch.Size([128100, 768]) in the checkpoint and torch.Size([30522, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DebertaV2ForMaskedLM.from_pretrained(\"microsoft/deberta-v3-base\", vocab_size=tokenizer.vocab_size, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data by max_lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_POS_EMBD = model.config.max_position_embeddings\n",
    "WINDOW = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def _tokenizer(sample, tokenizer, window, max_lenght):\n",
    "\n",
    "    tkn_text = tokenizer(sample['text'], add_special_tokens=False)\n",
    "\n",
    "    dev_train_dataset = []\n",
    "    for i in range(0, len(tkn_text['input_ids']), window):\n",
    "        dev_train_dataset.append(tkn_text['input_ids'][i:max_lenght+i])\n",
    "\n",
    "    return {k: v for k, v in enumerate(dev_train_dataset)}\n",
    "\n",
    "partial_tokenizer = partial(_tokenizer, tokenizer=tokenizer, window=WINDOW, max_lenght=MAX_POS_EMBD-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ni! Ni! Sou o Solstag, também conhecido por Ale. Moro em São Paulo, onde faço pesquisa e dou aulas, contribuo com a Wikiversidade e modestamente com a Wikipédia. Participo de alguns mutirões da Wikimedia Brasil, como: Cidades Sustentáveis e suas ramificações Arquigrafia Bienal Internacional de Arte de São Paulo Fora do Eixo Museu Afro Digital Wikimedia Foundation Universidade de São Paulo e suas ramificações São Paulo Colabora Sexta poética Reverberações Wikimedia:Bugzilla Mutirões dos quais já participei: Campus Party Sesc Pompéia Também sou admin nesta wiki, caso necessite de alguma ação administrativa pode me consultar. Em geral também estou no canal de IRC; acompanho cada vez menos a lista de discussão. /Várzea Tarefas gerais arquivar de maneira clara a página Wikimedia Brasil, trazida do Meta, e as suas subpáginas Onde estamos agora?',\n",
       " 'Mudanças na PP Gostei, apenas penso que os \"projetos irmãos\" deveriam aparecer depois dos \"projetos de conteúdo\". Abraços, Pietro Roveri 16h16min de 3 de agosto de 2010 (UTC) Outra coisa, por que o histórico não mostra quem fez as alterações? Pietro Roveri 16h18min de 3 de agosto de 2010 (UTC)',\n",
       " '29-30/10: Participamos da Conferência sobre Recursos Educacionais Abertos na Direito GV. 20/09: Encontro virtual discute evento no SESC em novembro, como atrair mais voluntários e situação do capítulo. 25-28/08: Participação brasileira no Wikimania 16/07: Encontro virtual discute Wikimania, envolvimento e capítulo. 09/06: Respondida colaborativamente entrevista sobre estatísticas da Wikipédia. 31/05: O mutirão Sexta poética prepara coletânea impressa de poemas entitulada \"Poema sem fim e por aí vai\". 28/05: Respondida entrevista para imprensa sobre a mudança de licença nos projetos Wikimedia. 19/05: Realizado diálogo com Comitê de Capítulos da Fundação Wikimedia. 06/05: Reunião via IRC gera relato do Encontro de capítulos 2009 em Berlim. 17/04: Encontro com IPTV Cultura, leia como foi.',\n",
       " '2008-2010 Para atividades de 2008 até Julho de 2010, consulte os Chapter Reports. 2010 Agosto dia 04: Encontro sobre Wikis no Conselho Nacional de Justiça Setembro dia 23: Apresentação sobre wiki no Superior Tribunal de Justiça (STJ), sobre o ambiente Wiki (contextualização do IBM-PC/Windows a WEB 2.0, a wikipédia, a Wikimedia Foundation e o software Mediawiki) e o E-learning baseado em Wiki. Veja aqui o pdf. dia 29: Reunião com Prof. Arthur Azevedo do Depto. de Matemática da Universidade de Brasília (Embaixadores) Outubro dia 21: feito o pedido de ativação do liquidthreads para esta wiki dia 22, às 16h e 19h: encontro e oficina em Guarulhos durante a feira do Instituto Federal (CEFET), junto com a A-Sol dia 28, às 19h: encontro e oficina sobre MediaWiki na Tech Talk da Casa de Cultura Digital Novembro dia 9, às 18h30: plenária de lançamento do Fórum Social de São Paulo dia 12: início da apresentação de mensagens para a captação anual de doações nos projetos da Wikimedia Foundation dias 14 a 17: oficinas no Festival Cultura Digital Br 2011 dia 20: participação no INDI Mix do Mutirão Sexta poética dias 25 a 30: Carolina Rossini, da Wikimedia Foundation, estará no Brasil; aproveitaremos a oportunidade para realizar alguns encontros: Dezembro dia 18 2012 Fevereiro 29 - Observatório Wiki às 18h na Casa Fora do Eixo em São Paulo 10 - Encontro acidental na Casa Fora do Eixo em São Paulo 09 - Mini-encontro Cidades Sustentáveis na Casa Jaya em São Paulo 08 - Usos científicos dos dados da Wikipédia no IME-USP em São Paulo 06 a 12 - Campus Party em São Paulo Março 30 - Encontro de capítulos em Berlim 29 - 1ª Conferência da USP sobre Transparência e Controle Social em São Paulo 28 e 29 - Fórum de Política Latino-Americana sobre Recursos Educacionais Abertos (UNESCO) no Rio de Janeiro 23 - Encontro sobre Arquigrafia na FAU-Butantã em São Paulo 22 - Cocriação de informações para o desenvolvimento social em São Paulo 21 - Encontro com Prof. Henrique Parra na Unifesp Guarulhos sobre curso na Wikiversidade 21 - Encontro no Sesc Belenzinho para discussão sobre WikiBrasil em São Paulo 20 - oficina sobre edição de wiki na USP em São Paulo 13 - palestra para calouros da Escola Politécnica em São Paulo 12 - encontro com Fundação Editora UNESP em São Paulo 12 - oficina para capacitação dos voluntários da PoliGNU em São Paulo 12 - encontro sobre Arquigrafia na FAU-Maranhão em São Paulo 09 - WikiNatal 1 no Rio Grande do Norte 06 - WikiRio 8 no Rio de Janeiro 04 - WikiCuritiba 1 no Paraná 03 - Orientação de professores e embaixadores de campus em São Paulo 02 a 09 - Entrevistas com doadores 02 - WikiSampa 12 no SESC Pinheiros em São Paulo Abril 28 - FLISOL 2012 em Curitiba, São Paulo, Salvador e muitas outras cidades do Brasil 28 e 29 - Oficinas na Casa Fora do Eixo em São Paulo 23 a 28 - Semana de Software Livre da USP em São Paulo 25 - Wikipédia na academia na Escola de Engenharia de São Carlos, UF São Paulo 14 - 5ª Jornada da Computação UFU, Uberlândia, Minas Gerais 12 - Encontro com grupo Next da Fiocruz no Rio de Janeiro 10 - Oficina Cidades Sustentáveis no CCE_SP em São Paulo 01 - Encontro de capítulos em Berlim Maio 29 - Programa Wikipédia no Ensino e demonstração sobre a Wikipédia 26 - Mapeamento em Heliópolis 25 - Wikimedia Brasil e Programa Wikipédia no Ensino (\"em discussão\") 14 - Seminário Museus e Cidades Criativas, no Rio de Janeiro 12 - Mapeamento de comunidades do Jardim Ângela 09 - Oficina do MootiroMaps na vila Guacuri 05 e 06 - Atividades durante da Virada Cultural, em São Paulo 04 a 06 - I Festival das Juventudes em Vitória da Conquista, Bahia 04 - Mapeamento de comunidades do Jardim Ângela Junho 26 - Oficina Wiki Cidades Sustentáveis em São Paulo 21 - Encontro com a Bienal Internacional de Arte de São Paulo 15 - Acompanhamento do planejamento comunitário na Cidade Ipava 13 - Mapeamento na Vila Cruzeiro (RJ) 13 - Oficina com jovens da Cidade Ipava 11 - Mapeamento cartográfico no Jardim São Remo 10 - Mapeamento cartográfico no Jardim São Remo 09 - Oficina com jovens e encontro com lideranças comunitárias da Cidade Ipava 06 - Reunião com o Diretor Social do Teto 05 - Oficina de mapeamento do MootiroMaps em Barra Bonita para coordenadores de desenvolvimento da Casa de Cultura e Cidadania 04 - Reunião com Fábio das Aldeias Infantis SOS 03 - Oficina com jovens do projeto Galera.com no Morro dos Prazeres 02 - Mutirão de mapeamento do MootiroMaps em comunidade do Rio de Janeiro Julho 26 a 29 - Campus Party Recife 12 a 15 - Wikimania Agosto 13 - Oficina Wiki Cidades Sustentáveis em São Paulo 04 e 05 - Caravana até Paraty, Rio de Janeiro Setembro 28 a 30 - Seminário de Educação Empreendedora e Feira do Empreendedor do Sebrae-RS em Porto Alegre 27 - Oficina Wiki Cidades Sustentáveis em São Paulo 22 - Oficina de capacitação do Programa Wikipédia na Universidade no Rio de Janeiro 03 a 05 - Rio Info 2012, no Rio de Janeiro Outubro Outubro 24 - Oficina Wiki Cidades Sustentáveis na Semana de Gestão Ambiental da USP Leste, em São Paulo 12 a 14 - WikiBrasil 2012 08 a 09 - VI Congresso de Direito de Autor e Interesse Público em Curitiba Novembro A gente dormiu Dezembro 15 a 18 - Global Congress on Intellectual Property and the Public Interest no Rio de Janeiro Para atividades recentes, veja as Últimas Notícias.',\n",
       " 'Descrição da Wikimedia Brasil Parece que estamos tendo várias edições no últimos dias sobre a descrição da Wikimedia Brasil: Wikimedia Brasil é o capítulo - em formação - da Fundação Wikimedia no Brasil. Wikimedia Brasil é o capítulo - de fato - da Fundação Wikimedia no Brasil. Wikimedia Brasil é um movimento vinculado ao Movimento Wikimedia e ainda realiza a função de capítulo brasileiro da Wikimedia Foundation de forma improvisada, sem representação legal. Acho que é o caso de discutirmos aqui ou na lista de emails os pontos conflitantes para chegarmos a um consenso. --Tom 00:47, 19 October 2009 (UTC) Não, há um consenso, houve um vandalismo, foi revertido, acabou, somos um movimento que atua como o capítulo brasileiro. m 01:29, 19 October 2009 (UTC) Nova descrição da Wikimedia Brasil Precisamos alterar esta descrição para melhor recepcionar aos novos usuários que entram nesta página. Um porém, no entanto: apesar do Thomas e cia terem organizado o evento em 2008, as discussões sobre o capítulo são bem anteriores a ele, envolvendo diversos voluntários que não participam mais por não aceitar o modelo de mutirões e afins, tendo, inclusive, um Estatuto aceito. Porém do modo como o texto está construído, está como se tudo começasse com o Thomas (de quem sou amigo) e de quem estava com ele, e que os Mutirões surgiram como uma solução da comunidade (eu mesmo fui contra os Mutirões na época e o sou até hoje). Uma mudança de abordagem e até um resgate de diversos pontos das antigas discussões poderiam permitir que estes voluntários que inicialmente estavam envolvidos possam colaborar com o atual movimento. Ozymandias (discussão) 15h03min de 24 de outubro de 2012 (UTC)']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected bytes, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"c:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\utils\\py_utils.py\", line 1353, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"c:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3466, in _map_single\n    writer.write_batch(batch)\n  File \"c:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\arrow_writer.py\", line 553, in write_batch\n    schema = inferred_features.arrow_schema if self.pa_writer is None else self.schema\n  File \"c:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\features\\features.py\", line 1613, in arrow_schema\n    return pa.schema(self.type).with_metadata({\"huggingface\": json.dumps(hf_metadata)})\n  File \"c:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\features\\features.py\", line 1602, in type\n    return get_nested_type(self)\n  File \"c:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\features\\features.py\", line 1188, in get_nested_type\n    return pa.struct(\n  File \"pyarrow\\types.pxi\", line 3019, in pyarrow.lib.struct\n  File \"pyarrow\\types.pxi\", line 2325, in pyarrow.lib.field\n  File \"stringsource\", line 15, in string.from_py.__pyx_convert_string_from_py_std__in_string\nTypeError: expected bytes, int found\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(partial_tokenizer, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, num_proc\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\arrow_dataset.py:578\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    577\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 578\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    579\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    580\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    581\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\arrow_dataset.py:543\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    537\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    538\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    541\u001b[0m }\n\u001b[0;32m    542\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    544\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    545\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\arrow_dataset.py:3166\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3158\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSpawning \u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m processes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   3159\u001b[0m \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   3160\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3161\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3164\u001b[0m     desc\u001b[39m=\u001b[39m(desc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (num_proc=\u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3165\u001b[0m ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3166\u001b[0m     \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m iflatmap_unordered(\n\u001b[0;32m   3167\u001b[0m         pool, Dataset\u001b[39m.\u001b[39m_map_single, kwargs_iterable\u001b[39m=\u001b[39mkwargs_per_job\n\u001b[0;32m   3168\u001b[0m     ):\n\u001b[0;32m   3169\u001b[0m         \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m   3170\u001b[0m             shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\utils\\py_utils.py:1379\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m   1376\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1378\u001b[0m     \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m     [async_result\u001b[39m.\u001b[39mget(timeout\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m) \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\utils\\py_utils.py:1379\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1376\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1378\u001b[0m     \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m     [async_result\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m) \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[1;31mTypeError\u001b[0m: expected bytes, int found"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(partial_tokenizer, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorWithPadding(\n",
    "#     tokenizer=tok, padding=\"max_length\", max_length=MAX_LENGTH\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(DatasetKGC(train), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_lg_former",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
