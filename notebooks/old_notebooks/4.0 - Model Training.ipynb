{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DebertaV2Tokenizer, AutoTokenizer, DebertaV2ForMaskedLM\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import load_from_disk\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"../data/c4ai-wik-tokenized-aux\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/deberta_pt_tokenizer\")\n",
    "model = DebertaV2ForMaskedLM.from_pretrained(\n",
    "    \"microsoft/deberta-v3-base\",\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dev training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_train = dataset.train_test_split(test_size=5, seed=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "params = {\n",
    "    \"output_dir\": \"model/model_deberta\",\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"seed\": 42,\n",
    "    \"max_steps\": 10000,\n",
    "    \"logging_dir\": \"model/logs\",\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"logging_steps\": 100,\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 5_000,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"report_to\": \"tensorboard\",\n",
    "    \"ddp_find_unused_parameters\": False,\n",
    "    \"warmup_steps\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=params[\"output_dir\"],\n",
    "    per_device_train_batch_size=params[\"per_device_train_batch_size\"],\n",
    "    learning_rate=params[\"learning_rate\"],\n",
    "    seed=params[\"seed\"],\n",
    "    max_steps=params[\"max_steps\"],\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=params[\"logging_dir\"],\n",
    "    logging_strategy=params[\"logging_strategy\"],\n",
    "    logging_steps=params[\"logging_steps\"],\n",
    "    save_strategy=params[\"save_strategy\"],\n",
    "    save_steps=params[\"save_steps\"],\n",
    "    save_total_limit=params[\"save_total_limit\"],\n",
    "    # report_to=params['report_to'],\n",
    "    # push to hub parameters\n",
    "    # push_to_hub=True,\n",
    "    # hub_strategy=\"every_save\",\n",
    "    # hub_model_id=script_args.repository_id,\n",
    "    # hub_token=script_args.hf_hub_token,\n",
    "    # pretraining\n",
    "    ddp_find_unused_parameters=params[\"ddp_find_unused_parameters\"],\n",
    "    warmup_steps=params[\"warmup_steps\"],\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15,\n",
    "    pad_to_multiple_of=8,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "model = model = DebertaV2ForMaskedLM.from_pretrained(\n",
    "    \"microsoft/deberta-v3-xsmall\",\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dev_train[\"test\"],\n",
    "    eval_dataset=dev_train[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,\n",
    "    # preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../model/model_deberta\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_lg_former",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
