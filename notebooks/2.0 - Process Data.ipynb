{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaV2Tokenizer, AutoTokenizer, DebertaV2ForMaskedLM\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_carolina = load_from_disk(\"../data/corpus-carolina\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    corpus: Dataset({\n",
       "        features: ['meta', 'text'],\n",
       "        num_rows: 2107045\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_carolina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset brwac/default to C:/Users/Matheus/.cache/huggingface/datasets/brwac/default-data_dir=..%2Fdata%2Fbrwac/1.0.0/cd622d293b6b90355cb6cb412bd9673902d6b9ce11d05b096a509f3aa40a0317...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset brwac downloaded and prepared to C:/Users/Matheus/.cache/huggingface/datasets/brwac/default-data_dir=..%2Fdata%2Fbrwac/1.0.0/cd622d293b6b90355cb6cb412bd9673902d6b9ce11d05b096a509f3aa40a0317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.36s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_brwac = load_dataset(\"brwac\", data_dir=\"../data/brwac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "dataset_brwac.save_to_disk(\"../data/brwac_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraphs': [['Conteúdo recente'],\n",
       "  ['ESPUMA MARROM CHAMADA \"NINGUÉM MERECE\"'],\n",
       "  ['31 de Agosto de 2015, 7:07 , por paulo soavinski - | No one following this article yet.'],\n",
       "  ['Visualizado 202 vezes'],\n",
       "  ['JORNAL ELETRÔNICO DA ILHA DO MEL'],\n",
       "  ['Uma espuma marrom escuro tem aparecido com frequência na Praia de Fora.',\n",
       "   'Na faixa de areia ela aparece disseminada e não chama muito a atenção.',\n",
       "   'No Buraco do Aipo, com muitas pedras, ela aparece concentrada.',\n",
       "   'É fácil saber que esta espuma estranha está lá, quando venta.',\n",
       "   'Pequenos algodões de espuma começam a flutuar no espaço, pertinho da Praia do Saquinho.',\n",
       "   'Quem pode ajudar na coleta deste material, envio a laboratório renomado e pagamento de análises, favor entrar em contato com o site.']]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_brwac[\"train\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ds_train \u001b[39m=\u001b[39m dataset_brwac\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39m\u001b[39mtrain[:1\u001b[39m\u001b[39m%\u001b[39m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "ds_train = dataset_brwac.split(\"train[:1%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"c:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\utils\\py_utils.py\", line 1353, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"c:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3449, in _map_single\n    batch = apply_function_on_filtered_inputs(\n  File \"c:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3330, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"C:\\Users\\Matheus\\AppData\\Local\\Temp\\ipykernel_2792\\2268061429.py\", line 2, in <lambda>\nTypeError: list indices must be integers or slices, not str\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset_brwac[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m      2\u001b[0m     \u001b[39mlambda\u001b[39;49;00m example: {\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(\u001b[39msum\u001b[39;49m(example[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mparagraphs\u001b[39;49m\u001b[39m\"\u001b[39;49m], []))]},\n\u001b[0;32m      3\u001b[0m     num_proc\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m     batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      5\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\arrow_dataset.py:578\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    577\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 578\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    579\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    580\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    581\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\arrow_dataset.py:543\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    537\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    538\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    541\u001b[0m }\n\u001b[0;32m    542\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    544\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    545\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\arrow_dataset.py:3166\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3158\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSpawning \u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m processes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   3159\u001b[0m \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   3160\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3161\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3164\u001b[0m     desc\u001b[39m=\u001b[39m(desc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (num_proc=\u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3165\u001b[0m ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3166\u001b[0m     \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m iflatmap_unordered(\n\u001b[0;32m   3167\u001b[0m         pool, Dataset\u001b[39m.\u001b[39m_map_single, kwargs_iterable\u001b[39m=\u001b[39mkwargs_per_job\n\u001b[0;32m   3168\u001b[0m     ):\n\u001b[0;32m   3169\u001b[0m         \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m   3170\u001b[0m             shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\utils\\py_utils.py:1379\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m   1376\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1378\u001b[0m     \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m     [async_result\u001b[39m.\u001b[39mget(timeout\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m) \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\datasets\\utils\\py_utils.py:1379\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1376\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1378\u001b[0m     \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m     [async_result\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m) \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\Longformer-PT\\venv_lg_former\\lib\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "dataset_brwac[\"train\"].map(\n",
    "    lambda example: {\"text\": [\"\\n\".join(sum(example[\"text\"][\"paragraphs\"], []))]},\n",
    "    num_proc=4,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Conteúdo recente\\nESPUMA MARROM CHAMADA \"NINGUÉM MERECE\"\\n31 de Agosto de 2015, 7:07 , por paulo soavinski - | No one following this article yet.\\nVisualizado 202 vezes\\nJORNAL ELETRÔNICO DA ILHA DO MEL\\nUma espuma marrom escuro tem aparecido com frequência na Praia de Fora.\\nNa faixa de areia ela aparece disseminada e não chama muito a atenção.\\nNo Buraco do Aipo, com muitas pedras, ela aparece concentrada.\\nÉ fácil saber que esta espuma estranha está lá, quando venta.\\nPequenos algodões de espuma começam a flutuar no espaço, pertinho da Praia do Saquinho.\\nQuem pode ajudar na coleta deste material, envio a laboratório renomado e pagamento de análises, favor entrar em contato com o site.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\n\".join(sum(dataset_brwac[\"train\"][0][\"text\"][\"paragraphs\"], []))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(dataset, batch_size=10_000):\n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        text = dataset[i : i + batch_size][\"text\"]\n",
    "        text = [ftfy.fix_text(t) for t in text]\n",
    "        yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 52.0/52.0 [00:00<?, ?B/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 474/474 [00:00<?, ?B/s] \n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 1.43MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 982kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6/211 [01:56<1:01:22, 17.96s/it]"
     ]
    }
   ],
   "source": [
    "new_tokenizer = tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(dataset_carolina['corpus']), vocab_size=tokenizer.vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer.save_pretrained(\"../models/deberta-ptbr-tokenizer-carolina\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/deberta-ptbr-tokenizer\")\n",
    "model = DebertaV2ForMaskedLM.from_pretrained(\"microsoft/deberta-v3-base\", vocab_size=tokenizer.vocab_size, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data by max_lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_POS_EMBD = model.config.max_position_embeddings\n",
    "WINDOW = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def _tokenizer(sample, tokenizer, window, max_lenght):\n",
    "    tkn_text = tokenizer(sample['text'], add_special_tokens=False)\n",
    "\n",
    "    dev_train_dataset = []\n",
    "\n",
    "    for sample in tkn_text['input_ids']:\n",
    "        for i in range(0, len(sample), window):\n",
    "            dev_train_dataset.append(tokenizer.decode(sample[i:max_lenght+i]).strip())\n",
    "    return {'text': dev_train_dataset}\n",
    "\n",
    "def _tokenizer_aux(sample, tokenizer):\n",
    "    tkn_text = tokenizer(sample['text'], return_special_tokens_mask=True)\n",
    "    return tkn_text\n",
    "\n",
    "partial_tokenizer = partial(_tokenizer, tokenizer=tokenizer, window=WINDOW, max_lenght=MAX_POS_EMBD-2)\n",
    "partial_tokenizer_aux = partial(_tokenizer_aux, tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(partial_tokenizer, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"../data/corpus-carolina-tokenized\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(partial_tokenizer_aux, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"../data/corpus-carolina-aux\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_lg_former",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
